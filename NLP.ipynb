{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/Corona_NLP_train.csv\", encoding='ISO-8859-1')\n",
    "test = pd.read_csv(\"data/Corona_NLP_test.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test, train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TweetAt'] = pd.to_datetime(df['TweetAt'], format=\"%d-%m-%Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44955 entries, 0 to 44954\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   UserName       44955 non-null  int64         \n",
      " 1   ScreenName     44955 non-null  int64         \n",
      " 2   Location       35531 non-null  object        \n",
      " 3   TweetAt        44955 non-null  datetime64[ns]\n",
      " 4   OriginalTweet  44955 non-null  object        \n",
      " 5   Sentiment      44955 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='TweetAt').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"TweetAt\", \"OriginalTweet\", \"Sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:00:00  <-- min date\n",
      "2020-04-14 00:00:00  <-- max date\n"
     ]
    }
   ],
   "source": [
    "print(str(df[\"TweetAt\"].min()) + \"  <-- min date\")\n",
    "print(str(df[\"TweetAt\"].max()) + \"  <-- max date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[[\"TweetAt\", \"OriginalTweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[\"length\"] = text[\"OriginalTweet\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = text['OriginalTweet'].str.findall(r'(@\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.insert(3,'nbr_mentions', mentions.apply(lambda x : len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@realDonaldTrump    0.014899\n",
       "@Tesco              0.010376\n",
       "@sainsburys         0.009418\n",
       "@BorisJohnson       0.008141\n",
       "@amazon             0.006385\n",
       "                      ...   \n",
       "@Billtony91         0.000053\n",
       "@BBCEngland         0.000053\n",
       "@parademag          0.000053\n",
       "@JohnFMauldin       0.000053\n",
       "@TartiiCat          0.000053\n",
       "Name: OriginalTweet, Length: 11163, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = mentions.explode().value_counts(True)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames_links(df):\n",
    "    df = re.sub('#[^\\s]+', '', df)\n",
    "    df = re.sub('@[^\\s]+','',df)\n",
    "    df = re.sub('http[^\\s]+','',df) \n",
    "    df = re.sub(' +', ' ', df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_without_tags\"] = df[\"OriginalTweet\"].apply(remove_usernames_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text_without_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buying hits City as anxious shoppers stock up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>One week everyone buying baby milk powder the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44950</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Breweries are making hand sanitizer over booze...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Breweries are making hand sanitizer over booze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44951</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Just scolded my dad who wanted to go supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Just scolded my dad who wanted to go supermark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44952</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>COVID-19 Special LIVE Phone-In Program with Sh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>COVID-19 Special LIVE Phone-In Program with Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44953</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>We may be saying goodbye to paper flyers soon,...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>We may be saying goodbye to paper flyers soon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44954</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Well new/used Rift S are going for $700.00 on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44955 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TweetAt                                      OriginalTweet  \\\n",
       "0     2020-03-02  TRENDING: New Yorkers encounter empty supermar...   \n",
       "1     2020-03-02  When I couldn't find hand sanitizer at Fred Me...   \n",
       "2     2020-03-02  Find out how you can protect yourself and love...   \n",
       "3     2020-03-02  #Panic buying hits #NewYork City as anxious sh...   \n",
       "4     2020-03-03  #toiletpaper #dunnypaper #coronavirus #coronav...   \n",
       "...          ...                                                ...   \n",
       "44950 2020-04-14  Breweries are making hand sanitizer over booze...   \n",
       "44951 2020-04-14  Just scolded my dad who wanted to go supermark...   \n",
       "44952 2020-04-14  COVID-19 Special LIVE Phone-In Program with Sh...   \n",
       "44953 2020-04-14  We may be saying goodbye to paper flyers soon,...   \n",
       "44954 2020-04-14  @TartiiCat Well new/used Rift S are going for ...   \n",
       "\n",
       "                Sentiment                                  text_without_tags  \n",
       "0      Extremely Negative  TRENDING: New Yorkers encounter empty supermar...  \n",
       "1                Positive  When I couldn't find hand sanitizer at Fred Me...  \n",
       "2      Extremely Positive  Find out how you can protect yourself and love...  \n",
       "3                Negative   buying hits City as anxious shoppers stock up...  \n",
       "4                 Neutral   One week everyone buying baby milk powder the...  \n",
       "...                   ...                                                ...  \n",
       "44950            Positive  Breweries are making hand sanitizer over booze...  \n",
       "44951  Extremely Negative  Just scolded my dad who wanted to go supermark...  \n",
       "44952            Positive  COVID-19 Special LIVE Phone-In Program with Sh...  \n",
       "44953            Positive  We may be saying goodbye to paper flyers soon,...  \n",
       "44954            Negative   Well new/used Rift S are going for $700.00 on...  \n",
       "\n",
       "[44955 rows x 4 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rogermauvois/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rogermauvois/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words_filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_without_tags'] = df['text_without_tags'].apply(lambda x: remove_stopwords(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text_without_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>trending : new yorkers encounter empty superma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>could n't find hand sanitizer fred meyer , tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>find protect loved ones ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>buying hits city anxious shoppers stock food &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>one week everyone buying baby milk powder next...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44950</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Breweries are making hand sanitizer over booze...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>breweries making hand sanitizer booze help fig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44951</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Just scolded my dad who wanted to go supermark...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>scolded dad wanted go supermarket walk walk to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44952</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>COVID-19 Special LIVE Phone-In Program with Sh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>covid-19 special live phone-in program shri k....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44953</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>We may be saying goodbye to paper flyers soon,...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>may saying goodbye paper flyers soon , thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44954</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>well new/used rift going $ 700.00 amazon rn al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44955 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TweetAt                                      OriginalTweet  \\\n",
       "0     2020-03-02  TRENDING: New Yorkers encounter empty supermar...   \n",
       "1     2020-03-02  When I couldn't find hand sanitizer at Fred Me...   \n",
       "2     2020-03-02  Find out how you can protect yourself and love...   \n",
       "3     2020-03-02  #Panic buying hits #NewYork City as anxious sh...   \n",
       "4     2020-03-03  #toiletpaper #dunnypaper #coronavirus #coronav...   \n",
       "...          ...                                                ...   \n",
       "44950 2020-04-14  Breweries are making hand sanitizer over booze...   \n",
       "44951 2020-04-14  Just scolded my dad who wanted to go supermark...   \n",
       "44952 2020-04-14  COVID-19 Special LIVE Phone-In Program with Sh...   \n",
       "44953 2020-04-14  We may be saying goodbye to paper flyers soon,...   \n",
       "44954 2020-04-14  @TartiiCat Well new/used Rift S are going for ...   \n",
       "\n",
       "                Sentiment                                  text_without_tags  \n",
       "0      Extremely Negative  trending : new yorkers encounter empty superma...  \n",
       "1                Positive  could n't find hand sanitizer fred meyer , tur...  \n",
       "2      Extremely Positive                          find protect loved ones ?  \n",
       "3                Negative  buying hits city anxious shoppers stock food &...  \n",
       "4                 Neutral  one week everyone buying baby milk powder next...  \n",
       "...                   ...                                                ...  \n",
       "44950            Positive  breweries making hand sanitizer booze help fig...  \n",
       "44951  Extremely Negative  scolded dad wanted go supermarket walk walk to...  \n",
       "44952            Positive  covid-19 special live phone-in program shri k....  \n",
       "44953            Positive      may saying goodbye paper flyers soon , thanks  \n",
       "44954            Negative  well new/used rift going $ 700.00 amazon rn al...  \n",
       "\n",
       "[44955 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\n",
    "    'Extremely Negative': -1,\n",
    "    'Negative': -0.5,\n",
    "    'Neutral': 0,\n",
    "    'Positive': 0.5,\n",
    "    'Extremely Positive': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, TextVectorization\n",
    "from tensorflow.keras import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text_without_tags\"].values\n",
    "y = df[\"Sentiment\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26747,), (11464,), (6744,))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_tokens = 20000\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cours/lib/python3.11/site-packages/keras/src/engine/training.py:3482\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3452\u001b[0m \n\u001b[1;32m   3453\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3486\u001b[0m     )\n\u001b[1;32m   3487\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3489\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3494\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3495\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TextVectorization(max_tokens = max_tokens, output_sequence_length = 50, name = \"text_vectorizer\"))\n",
    "model.add(Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim\n",
    "    # embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    # trainable=False, \n",
    "    # mask_zero=True,\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation=\"tanh\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss= \"\")\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "max_length = 600 # Longueur max des entrÃ©es (seul 5% des reviews font plus de 600 mots)\n",
    "max_tokens = 20000 # Limite le vocabulaire aux 20_000 mots les plus utilisÃ©s\n",
    "text_vectorization = layers.TextVectorization(\n",
    " max_tokens=max_tokens,\n",
    " output_mode=\"int\", # Les sorties sont des sÃ©quences de mots encodÃ©es comme des entiers\n",
    " output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
